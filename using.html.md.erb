---
title: Using PCF Log Search
owner: London Services
---

This topic describes how to get started with Kibana, the front-end component of PCF Log Search. Kibana is a web application that lets you search and filter system logs, design visualizations of saved searches, and create dashboards.

##<a id="login"></a>Log in to Kibana

1. From the **Installation Dashboard** in Ops Manager, click on the **Log Search** tile. 

	![Tile](tile.png)

1. Under the **Credentials** tab, open and record the **Kibana Credentials**.

	![Credentials](creds.png)

1. Navigate to `https://logsearch.YOUR-SYSTEM-DOMAIN` and log in to Kibana using the credentials. 

1. (First time only) When prompted to configure an index pattern, enter `logstash-*` for the **Index name or pattern** and `@timestamp` for the **Time-field name**. 

	![Kibana Index pattern config screenshot](images/index-pattern-config.png) 

##<a id="get-started"></a> Get Started with Kibana 

###<a id="tags"></a> Understand Log Search Tags

Log Search receives data from other tiles in JSON format. It organizes this data into searchable fields based on the JSON keys, and also aggregates fields under custom tags. Log Search attaches these tags when it recognizes that different tile logs use different keys to refer to the same type of data. For instance, one tile may specify the timestamp under a `Timestamp` field, while another specifies this value under a `T` field. Log Search recognizes both of these values as a timestamp, so it attaches the `@timestamp` tag.

Log Search does this for other kinds of data as well. See the [Log Search Tags Dictionary](./search-guide.html) topic for the full list of tags generated by Log Search.

###<a id="searches"></a> Filter, Search, and Visualize

The following list describes what you can do with the Kibana component of PCF Log Search:

* [Filter Log Data by Field](https://www.elastic.co/guide/en/kibana/current/discover.html#field-filter): You can filter log data based on tags generated by Log Search or any keys within the JSON logs themselves. The **Available Fields** list on the left side of the **Discover** page lists the Log Search tags first, followed by the parsed log keys.

* Change the [Time Scale](https://www.elastic.co/guide/en/kibana/current/discover.html#set-time-filter) and [Refresh Interval](https://www.elastic.co/guide/en/kibana/current/discover.html#auto-refresh): By default, the time scale is set to the last **15 minutes**. 

* [Search Log Data](https://www.elastic.co/guide/en/kibana/current/discover.html#search): You can further refine your results from any filter or time span using the search bar at the top of the **Discover** page. You can also search against any field by typing your query in the following format: `FIELD:VALUE`. For example: `@source.ip:10.0.16.21`.

* [Design Data Visualizations](https://www.elastic.co/guide/en/kibana/current/visualize.html): You can create visualizations such as **Data Table**, **Line Chart**, and **Vertical Bar Chart**. You can also [customize your visualizations](https://www.elastic.co/guide/en/kibana/current/visualize.html#aggregation-builder). For example, you can tailor the x and y axis of a Vertical Bar Chart using bucket aggregations and metric aggregations. 

* [Create a Dashboard](https://www.elastic.co/guide/en/kibana/current/dashboard.html): You can create a dashboard to display multiple visualizations and saved searches. You can also apply filters to your dashboard, which affects all the displayed panes. For instance, using the time filter applies your time changes to every displayed visualization and saved search. 

For more information, view the [Kibana documentation](https://www.elastic.co/guide/en/kibana/current/index.html).

###<a id="tags"></a> Forwarding UAA data to Splunk

A common requirement is to forward log data to a central auditing system. PCF Log Search can forward some or all the data it recieves to an external service like Splunk as JSON.

#### Configure Splunk
* <p class="note"><strong>Note</strong>: UDP is strongly encouraged to prevent network communication problems with the Splunk Network input prevent data being indexed by Log Search </p>
* Documentation for configuring a network input is available at http://docs.splunk.com/Documentation/Splunk/latest/Data/Monitornetworkports
	* When prompted, select `_json` as the Source Type

#### Configure Log Search
* Configure Experimental > Custom Logstash outputs with the Splunk UDP network input configured above.
* A conditional statement can be used to filter the data sent to Splunk
	* eg: `if [@source][program] == "uaa" {`
* Example Configuration

		if [@source][program] == "uaa" {
		  udp {
		    host => "splunk-ip-or-dns"
		    port => "splunk-udp-port-number"
		 }
		}

#### Network Configuration
* Data will be sent via UDP from the Log Search Log parser VMs to the Splunk Network input on the configured port.
* Firewall rules must allow:
   * Outgoing traffic from the Log Search Log parser VMs on the configured port
   * Incoming traffic to the Splunk installation on the configured port.
   
#### Checking that forwarding is working

Once your have configured forwarding successfully, the data will be available in both Log Search and Splunk.

![UAA log in Log Search > Kibana screenshot](images/UAA-log-data-in-Kibana.png) 

![UAA log in Splunk screenshot](images/UAA-log-data-in-Splunk.png) 
